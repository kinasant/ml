{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":420,"sourceType":"datasetVersion","datasetId":19}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-29T04:17:34.647008Z","iopub.execute_input":"2024-09-29T04:17:34.647539Z","iopub.status.idle":"2024-09-29T04:17:34.683173Z","shell.execute_reply.started":"2024-09-29T04:17:34.647493Z","shell.execute_reply":"2024-09-29T04:17:34.682049Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"/kaggle/input/iris/Iris.csv\n/kaggle/input/iris/database.sqlite\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/iris/Iris.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-29T04:24:17.313254Z","iopub.execute_input":"2024-09-29T04:24:17.313711Z","iopub.status.idle":"2024-09-29T04:24:17.340608Z","shell.execute_reply.started":"2024-09-29T04:24:17.313666Z","shell.execute_reply":"2024-09-29T04:24:17.339450Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"X = df.drop(\"Species\",axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T04:28:07.247125Z","iopub.execute_input":"2024-09-29T04:28:07.247613Z","iopub.status.idle":"2024-09-29T04:28:07.254540Z","shell.execute_reply.started":"2024-09-29T04:28:07.247567Z","shell.execute_reply":"2024-09-29T04:28:07.253145Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"X = X.iloc[:,1:]","metadata":{"execution":{"iopub.status.busy":"2024-09-29T04:35:10.440203Z","iopub.execute_input":"2024-09-29T04:35:10.440666Z","iopub.status.idle":"2024-09-29T04:35:10.446542Z","shell.execute_reply.started":"2024-09-29T04:35:10.440621Z","shell.execute_reply":"2024-09-29T04:35:10.445330Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"y = df[\"Species\"]","metadata":{"execution":{"iopub.status.busy":"2024-09-29T04:28:08.343230Z","iopub.execute_input":"2024-09-29T04:28:08.343663Z","iopub.status.idle":"2024-09-29T04:28:08.348968Z","shell.execute_reply.started":"2024-09-29T04:28:08.343623Z","shell.execute_reply":"2024-09-29T04:28:08.347629Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"y = y.map(lambda x: ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'].index(x))","metadata":{"execution":{"iopub.status.busy":"2024-09-29T04:34:39.619283Z","iopub.execute_input":"2024-09-29T04:34:39.619745Z","iopub.status.idle":"2024-09-29T04:34:39.626034Z","shell.execute_reply.started":"2024-09-29T04:34:39.619698Z","shell.execute_reply":"2024-09-29T04:34:39.624678Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"A2. Use cross-validation techniques (RandomizedSearchCV()) technique to tune the \nhyperparameters for your perceptron and MLP networks.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import Perceptron","metadata":{"execution":{"iopub.status.busy":"2024-09-29T04:17:34.718432Z","iopub.execute_input":"2024-09-29T04:17:34.719654Z","iopub.status.idle":"2024-09-29T04:17:34.727687Z","shell.execute_reply.started":"2024-09-29T04:17:34.719602Z","shell.execute_reply":"2024-09-29T04:17:34.726441Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Define hyperparameter search spaces\nparam_distributions_perceptron = {\n    'penalty': ['l2', 'l1', 'elasticnet'],\n    'alpha': [0.002, 0.001, 0.01, 0.1],\n    'max_iter': [100, 500, 1000],\n    'tol': [1e-3, 1e-4, 1e-5]\n}\n\nparam_distributions_mlp = {\n    'hidden_layer_sizes': [(50,)],\n    'activation': ['relu', 'tanh', 'logistic'],\n    'solver': ['lbfgs', 'sgd', 'adam'],\n    'alpha': [0.002, 0.001, 0.01],\n    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n    'max_iter': [100, 500, 1000],\n    'tol': [1e-3, 1e-4, 1e-5]\n}\n\n# Define your models\nperceptron = Perceptron(random_state=0)\nmlp = MLPClassifier(hidden_layer_sizes=(2,), activation='logistic', solver='lbfgs', max_iter=1000)\n\n# Create RandomizedSearchCV objects\nrandom_search_perceptron = RandomizedSearchCV(\n    estimator=perceptron,\n    param_distributions=param_distributions_perceptron,\n    n_iter=10,  # Number of parameter settings to sample\n    cv=5,  # Cross-validation folds\n    scoring='accuracy',  # Evaluation metric\n    verbose=2,  # Output verbosity level\n    random_state=0\n)\n\nrandom_search_mlp = RandomizedSearchCV(\n    estimator=mlp,\n    param_distributions=param_distributions_mlp,\n    n_iter=10,\n    cv=5,\n    scoring='accuracy',\n    verbose=2,\n    random_state=0\n)\n\n# Fit the models with RandomizedSearchCV\nrandom_search_perceptron.fit(X, y)\nrandom_search_mlp.fit(X, y)\n\n# Print best hyperparameters and scores\nprint(\"Best Perceptron Hyperparameters:\", random_search_perceptron.best_params_)\nprint(\"Best Perceptron Accuracy:\", random_search_perceptron.best_score_)\nprint(\"\\n\")\nprint(\"Best MLP Hyperparameters:\", random_search_mlp.best_params_)\nprint(\"Best MLP Accuracy:\", random_search_mlp.best_score_)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T04:19:01.121827Z","iopub.execute_input":"2024-09-29T04:19:01.122836Z","iopub.status.idle":"2024-09-29T04:19:13.177707Z","shell.execute_reply.started":"2024-09-29T04:19:01.122785Z","shell.execute_reply":"2024-09-29T04:19:13.176628Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 10 candidates, totalling 50 fits\n[CV] END ...alpha=0.01, max_iter=1000, penalty=l1, tol=1e-05; total time=   0.0s\n[CV] END ...alpha=0.01, max_iter=1000, penalty=l1, tol=1e-05; total time=   0.0s\n[CV] END ...alpha=0.01, max_iter=1000, penalty=l1, tol=1e-05; total time=   0.0s\n[CV] END ...alpha=0.01, max_iter=1000, penalty=l1, tol=1e-05; total time=   0.0s\n[CV] END ...alpha=0.01, max_iter=1000, penalty=l1, tol=1e-05; total time=   0.0s\n[CV] END ..alpha=0.002, max_iter=500, penalty=l2, tol=0.0001; total time=   0.0s\n[CV] END ..alpha=0.002, max_iter=500, penalty=l2, tol=0.0001; total time=   0.0s\n[CV] END ..alpha=0.002, max_iter=500, penalty=l2, tol=0.0001; total time=   0.0s\n[CV] END ..alpha=0.002, max_iter=500, penalty=l2, tol=0.0001; total time=   0.0s\n[CV] END ..alpha=0.002, max_iter=500, penalty=l2, tol=0.0001; total time=   0.0s\n[CV] END ..alpha=0.002, max_iter=100, penalty=l1, tol=0.0001; total time=   0.0s\n[CV] END ..alpha=0.002, max_iter=100, penalty=l1, tol=0.0001; total time=   0.0s\n[CV] END ..alpha=0.002, max_iter=100, penalty=l1, tol=0.0001; total time=   0.0s\n[CV] END ..alpha=0.002, max_iter=100, penalty=l1, tol=0.0001; total time=   0.0s\n[CV] END ..alpha=0.002, max_iter=100, penalty=l1, tol=0.0001; total time=   0.0s\n[CV] END .....alpha=0.1, max_iter=100, penalty=l2, tol=1e-05; total time=   0.0s\n[CV] END .....alpha=0.1, max_iter=100, penalty=l2, tol=1e-05; total time=   0.0s\n[CV] END .....alpha=0.1, max_iter=100, penalty=l2, tol=1e-05; total time=   0.0s\n[CV] END .....alpha=0.1, max_iter=100, penalty=l2, tol=1e-05; total time=   0.0s\n[CV] END .....alpha=0.1, max_iter=100, penalty=l2, tol=1e-05; total time=   0.0s\n[CV] END alpha=0.01, max_iter=100, penalty=elasticnet, tol=1e-05; total time=   0.0s\n[CV] END alpha=0.01, max_iter=100, penalty=elasticnet, tol=1e-05; total time=   0.0s\n[CV] END alpha=0.01, max_iter=100, penalty=elasticnet, tol=1e-05; total time=   0.0s\n[CV] END alpha=0.01, max_iter=100, penalty=elasticnet, tol=1e-05; total time=   0.0s\n[CV] END alpha=0.01, max_iter=100, penalty=elasticnet, tol=1e-05; total time=   0.0s\n[CV] END ...alpha=0.01, max_iter=500, penalty=l1, tol=0.0001; total time=   0.0s\n[CV] END ...alpha=0.01, max_iter=500, penalty=l1, tol=0.0001; total time=   0.0s\n[CV] END ...alpha=0.01, max_iter=500, penalty=l1, tol=0.0001; total time=   0.0s\n[CV] END ...alpha=0.01, max_iter=500, penalty=l1, tol=0.0001; total time=   0.0s\n[CV] END ...alpha=0.01, max_iter=500, penalty=l1, tol=0.0001; total time=   0.0s\n[CV] END ...alpha=0.001, max_iter=100, penalty=l1, tol=0.001; total time=   0.0s\n[CV] END ...alpha=0.001, max_iter=100, penalty=l1, tol=0.001; total time=   0.0s\n[CV] END ...alpha=0.001, max_iter=100, penalty=l1, tol=0.001; total time=   0.0s\n[CV] END ...alpha=0.001, max_iter=100, penalty=l1, tol=0.001; total time=   0.0s\n[CV] END ...alpha=0.001, max_iter=100, penalty=l1, tol=0.001; total time=   0.0s\n[CV] END ..alpha=0.001, max_iter=1000, penalty=l2, tol=0.001; total time=   0.0s\n[CV] END ..alpha=0.001, max_iter=1000, penalty=l2, tol=0.001; total time=   0.0s\n[CV] END ..alpha=0.001, max_iter=1000, penalty=l2, tol=0.001; total time=   0.0s\n[CV] END ..alpha=0.001, max_iter=1000, penalty=l2, tol=0.001; total time=   0.0s\n[CV] END ..alpha=0.001, max_iter=1000, penalty=l2, tol=0.001; total time=   0.0s\n[CV] END .....alpha=0.1, max_iter=500, penalty=l1, tol=1e-05; total time=   0.0s\n[CV] END .....alpha=0.1, max_iter=500, penalty=l1, tol=1e-05; total time=   0.0s\n[CV] END .....alpha=0.1, max_iter=500, penalty=l1, tol=1e-05; total time=   0.0s\n[CV] END .....alpha=0.1, max_iter=500, penalty=l1, tol=1e-05; total time=   0.0s\n[CV] END .....alpha=0.1, max_iter=500, penalty=l1, tol=1e-05; total time=   0.0s\n[CV] END ...alpha=0.002, max_iter=500, penalty=l2, tol=1e-05; total time=   0.0s\n[CV] END ...alpha=0.002, max_iter=500, penalty=l2, tol=1e-05; total time=   0.0s\n[CV] END ...alpha=0.002, max_iter=500, penalty=l2, tol=1e-05; total time=   0.0s\n[CV] END ...alpha=0.002, max_iter=500, penalty=l2, tol=1e-05; total time=   0.0s\n[CV] END ...alpha=0.002, max_iter=500, penalty=l2, tol=1e-05; total time=   0.0s\nFitting 5 folds for each of 10 candidates, totalling 50 fits\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=500, solver=lbfgs, tol=0.001; total time=   0.3s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=500, solver=lbfgs, tol=0.001; total time=   0.5s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=500, solver=lbfgs, tol=0.001; total time=   0.5s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=500, solver=lbfgs, tol=0.001; total time=   0.4s\n[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=500, solver=lbfgs, tol=0.001; total time=   0.2s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=500, solver=sgd, tol=0.0001; total time=   0.3s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=500, solver=sgd, tol=0.0001; total time=   0.3s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=500, solver=sgd, tol=0.0001; total time=   0.2s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=500, solver=sgd, tol=0.0001; total time=   0.3s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=500, solver=sgd, tol=0.0001; total time=   0.3s\n[CV] END activation=tanh, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=100, solver=sgd, tol=1e-05; total time=   0.1s\n[CV] END activation=tanh, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=100, solver=sgd, tol=1e-05; total time=   0.1s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END activation=tanh, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=100, solver=sgd, tol=1e-05; total time=   0.1s\n[CV] END activation=tanh, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=100, solver=sgd, tol=1e-05; total time=   0.1s\n[CV] END activation=tanh, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=100, solver=sgd, tol=1e-05; total time=   0.1s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=100, solver=sgd, tol=0.0001; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=100, solver=sgd, tol=0.0001; total time=   0.1s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=100, solver=sgd, tol=0.0001; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=100, solver=sgd, tol=0.0001; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=100, solver=sgd, tol=0.0001; total time=   0.1s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","output_type":"stream"},{"name":"stdout","text":"[CV] END activation=logistic, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=100, solver=lbfgs, tol=1e-05; total time=   0.1s\n[CV] END activation=logistic, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=100, solver=lbfgs, tol=1e-05; total time=   0.1s\n[CV] END activation=logistic, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=100, solver=lbfgs, tol=1e-05; total time=   0.1s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","output_type":"stream"},{"name":"stdout","text":"[CV] END activation=logistic, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=100, solver=lbfgs, tol=1e-05; total time=   0.1s\n[CV] END activation=logistic, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=100, solver=lbfgs, tol=1e-05; total time=   0.1s\n[CV] END activation=tanh, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=sgd, tol=1e-05; total time=   0.6s\n[CV] END activation=tanh, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=sgd, tol=1e-05; total time=   0.8s\n[CV] END activation=tanh, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=sgd, tol=1e-05; total time=   0.8s\n[CV] END activation=tanh, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=sgd, tol=1e-05; total time=   0.8s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END activation=tanh, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=sgd, tol=1e-05; total time=   0.9s\n[CV] END activation=relu, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=1000, solver=lbfgs, tol=0.001; total time=   0.1s\n[CV] END activation=relu, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=1000, solver=lbfgs, tol=0.001; total time=   0.1s\n[CV] END activation=relu, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=1000, solver=lbfgs, tol=0.001; total time=   0.1s\n[CV] END activation=relu, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=1000, solver=lbfgs, tol=0.001; total time=   0.1s\n[CV] END activation=relu, alpha=0.002, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=1000, solver=lbfgs, tol=0.001; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=100, solver=lbfgs, tol=0.001; total time=   0.1s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","output_type":"stream"},{"name":"stdout","text":"[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=100, solver=lbfgs, tol=0.001; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=100, solver=lbfgs, tol=0.001; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=100, solver=lbfgs, tol=0.001; total time=   0.1s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","output_type":"stream"},{"name":"stdout","text":"[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=100, solver=lbfgs, tol=0.001; total time=   0.1s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=1000, solver=adam, tol=0.0001; total time=   0.5s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=1000, solver=adam, tol=0.0001; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=1000, solver=adam, tol=0.0001; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=1000, solver=adam, tol=0.0001; total time=   0.0s\n[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=1000, solver=adam, tol=0.0001; total time=   0.0s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=lbfgs, tol=0.001; total time=   0.1s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=lbfgs, tol=0.001; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=lbfgs, tol=0.001; total time=   0.2s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=lbfgs, tol=0.001; total time=   0.4s\n[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=1000, solver=lbfgs, tol=0.001; total time=   0.4s\nBest Perceptron Hyperparameters: {'tol': 1e-05, 'penalty': 'l1', 'max_iter': 1000, 'alpha': 0.01}\nBest Perceptron Accuracy: 0.6799999999999999\n\n\nBest MLP Hyperparameters: {'tol': 0.001, 'solver': 'lbfgs', 'max_iter': 500, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (50,), 'alpha': 0.01, 'activation': 'tanh'}\nBest MLP Accuracy: 1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"perceptron = Perceptron(**random_search_perceptron.best_params_,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T04:41:42.332489Z","iopub.execute_input":"2024-09-29T04:41:42.333061Z","iopub.status.idle":"2024-09-29T04:41:42.339729Z","shell.execute_reply.started":"2024-09-29T04:41:42.333005Z","shell.execute_reply":"2024-09-29T04:41:42.338435Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import Perceptron\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Define classifiers\nclassifiers = {\n    'Perceptron': Perceptron(**random_search_perceptron.best_params_,random_state=0),\n    'MLP': MLPClassifier(**random_search_mlp.best_params_,random_state=0),\n    'SVM': SVC(random_state=0),\n    'Decision Tree': DecisionTreeClassifier(random_state=0),\n    'Random Forest': RandomForestClassifier(random_state=0),\n    'CatBoost': CatBoostClassifier(random_state=0, verbose=False),\n    'AdaBoost': AdaBoostClassifier(random_state=0),\n    'XGBoost': XGBClassifier(random_state=0, objective='multi:softprob'),  # For multi-class\n    'Naive Bayes': GaussianNB()\n}\n\n# Define performance metrics\nmetrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n\n# Initialize results dataframe\nresults = pd.DataFrame(columns=['Classifier'] + metrics)\n\n# Train and evaluate each classifier\nfor name, classifier in classifiers.items():\n    # Train the classifier\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n\n    # Calculate performance metrics (correcting for multi-class)\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred,average='macro') # Weighted averaging for multi-class\n    recall = recall_score(y_test, y_pred,average='macro' ) \n    f1 = f1_score(y_test, y_pred,average='macro') \n\n    # Append results to dataframe (using pandas.concat)\n    new_row = pd.DataFrame({'Classifier': [name],\n                              'Accuracy': [accuracy],\n                              'Precision': [precision],\n                              'Recall': [recall],\n                              'F1-score': [f1]})\n    results = pd.concat([results, new_row], ignore_index=True)\n\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T04:47:37.777054Z","iopub.execute_input":"2024-09-29T04:47:37.777554Z","iopub.status.idle":"2024-09-29T04:47:39.210582Z","shell.execute_reply.started":"2024-09-29T04:47:37.777503Z","shell.execute_reply":"2024-09-29T04:47:39.209178Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/tmp/ipykernel_30/4096976722.py:54: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  results = pd.concat([results, new_row], ignore_index=True)\n","output_type":"stream"},{"name":"stdout","text":"      Classifier  Accuracy  Precision    Recall  F1-score\n0     Perceptron  0.566667   0.374801  0.666667  0.479739\n1            MLP  0.966667   0.976190  0.944444  0.957351\n2            SVM  1.000000   1.000000  1.000000  1.000000\n3  Decision Tree  1.000000   1.000000  1.000000  1.000000\n4  Random Forest  1.000000   1.000000  1.000000  1.000000\n5       CatBoost  1.000000   1.000000  1.000000  1.000000\n6       AdaBoost  0.966667   0.976190  0.944444  0.957351\n7        XGBoost  1.000000   1.000000  1.000000  1.000000\n8    Naive Bayes  0.966667   0.976190  0.944444  0.957351\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}